"""
Upload SDTM AE domain and associated artifacts to AWS S3.

This script uploads all AE domain deliverables to S3 with:
- Timestamped directory structure for versioning
- MD5 checksum calculation
- Manifest file generation
- Upload verification
- Comprehensive logging
"""

import os
import json
import hashlib
from datetime import datetime
from pathlib import Path
import asyncio

# File mapping: local path -> S3 key suffix
FILES_TO_UPLOAD = {
    "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/ae_domain.csv": "ae_domain.csv",
    "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/ae_mapping_specification.json": "ae_mapping_spec.json",
    "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/ae_validation_report.json": "ae_validation_report.json",
    "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/AE_TRANSFORMATION_COMPREHENSIVE_REPORT.md": "AE_TRANSFORMATION_COMPREHENSIVE_REPORT.md",
    "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/AE_VALIDATION_EXECUTIVE_SUMMARY_FINAL.md": "AE_VALIDATION_EXECUTIVE_SUMMARY_FINAL.md"
}

# S3 Configuration
BUCKET = "s3dcri"
STUDY_ID = "MAXIS-08"
DOMAIN = "AE"
S3_PREFIX = f"processed/{STUDY_ID}/{DOMAIN}"


def calculate_md5(file_path: str) -> str:
    """Calculate MD5 checksum for a file."""
    md5_hash = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5_hash.update(chunk)
    return md5_hash.hexdigest()


def get_file_size(file_path: str) -> int:
    """Get file size in bytes."""
    return os.path.getsize(file_path)


def get_content_type(file_path: str) -> str:
    """Determine content type based on file extension."""
    ext = Path(file_path).suffix.lower()
    content_types = {
        '.csv': 'text/csv',
        '.json': 'application/json',
        '.md': 'text/markdown',
        '.txt': 'text/plain',
        '.pdf': 'application/pdf',
        '.xml': 'application/xml'
    }
    return content_types.get(ext, 'application/octet-stream')


async def verify_file_exists(file_path: str) -> bool:
    """Verify that a file exists."""
    return os.path.exists(file_path) and os.path.isfile(file_path)


async def main():
    """Main upload orchestration."""
    print("=" * 80)
    print("SDTM AE Domain S3 Upload Process")
    print("=" * 80)
    
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"\nüìÖ Upload Timestamp: {timestamp}")
    print(f"ü™£  Target Bucket: s3://{BUCKET}")
    print(f"üìÅ Study ID: {STUDY_ID}")
    print(f"üìä Domain: {DOMAIN}")
    
    # Step 1: Verify all files exist
    print("\n" + "=" * 80)
    print("STEP 1: Verifying Files")
    print("=" * 80)
    
    missing_files = []
    for local_path in FILES_TO_UPLOAD.keys():
        exists = await verify_file_exists(local_path)
        status = "‚úÖ EXISTS" if exists else "‚ùå MISSING"
        print(f"{status}: {Path(local_path).name}")
        if not exists:
            missing_files.append(local_path)
    
    if missing_files:
        print(f"\n‚ùå ERROR: {len(missing_files)} file(s) missing!")
        for f in missing_files:
            print(f"   - {f}")
        return
    
    print("\n‚úÖ All files verified successfully!")
    
    # Step 2: Calculate checksums and collect metadata
    print("\n" + "=" * 80)
    print("STEP 2: Calculating Checksums and Metadata")
    print("=" * 80)
    
    file_metadata = {}
    total_size = 0
    
    for local_path, s3_suffix in FILES_TO_UPLOAD.items():
        print(f"\nüìÑ Processing: {Path(local_path).name}")
        
        # Calculate metadata
        file_size = get_file_size(local_path)
        md5_checksum = calculate_md5(local_path)
        content_type = get_content_type(local_path)
        
        # Build S3 key
        s3_key = f"{S3_PREFIX}/{timestamp}/{s3_suffix}"
        s3_uri = f"s3://{BUCKET}/{s3_key}"
        
        file_metadata[s3_suffix] = {
            "local_path": local_path,
            "s3_key": s3_key,
            "s3_uri": s3_uri,
            "file_size_bytes": file_size,
            "file_size_kb": round(file_size / 1024, 2),
            "file_size_mb": round(file_size / (1024 * 1024), 2),
            "md5_checksum": md5_checksum,
            "content_type": content_type
        }
        
        total_size += file_size
        
        print(f"   Size: {file_metadata[s3_suffix]['file_size_kb']} KB")
        print(f"   MD5: {md5_checksum}")
        print(f"   Content-Type: {content_type}")
        print(f"   S3 URI: {s3_uri}")
    
    print(f"\nüìä Total size: {round(total_size / 1024, 2)} KB ({round(total_size / (1024 * 1024), 2)} MB)")
    
    # Step 3: Upload files to S3
    print("\n" + "=" * 80)
    print("STEP 3: Uploading Files to S3")
    print("=" * 80)
    
    upload_start_time = datetime.now()
    upload_results = []
    
    # Import the upload_to_s3 tool
    # Note: This will be handled by the agent's tool system
    print("\nüöÄ Initiating uploads...")
    
    for s3_suffix, metadata in file_metadata.items():
        print(f"\nüì§ Uploading: {s3_suffix}")
        print(f"   From: {metadata['local_path']}")
        print(f"   To: {metadata['s3_uri']}")
        
        # The actual upload will be performed by the upload_to_s3 tool
        # This script prepares the metadata and structure
        upload_results.append({
            "file": s3_suffix,
            "s3_key": metadata['s3_key'],
            "status": "pending"
        })
    
    # Step 4: Generate manifest
    print("\n" + "=" * 80)
    print("STEP 4: Generating Manifest")
    print("=" * 80)
    
    manifest = {
        "manifest_version": "1.0",
        "study_id": STUDY_ID,
        "domain": DOMAIN,
        "upload_timestamp": timestamp,
        "upload_datetime_iso": datetime.now().isoformat(),
        "bucket": BUCKET,
        "s3_prefix": S3_PREFIX,
        "s3_base_path": f"s3://{BUCKET}/{S3_PREFIX}/{timestamp}",
        "file_count": len(file_metadata),
        "total_size_bytes": total_size,
        "total_size_kb": round(total_size / 1024, 2),
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "files": file_metadata,
        "upload_metadata": {
            "uploader": "Data Loading Agent",
            "pipeline_phase": "Phase 7 - Data Warehouse Loading",
            "purpose": "Regulatory submission and downstream processing",
            "verification_method": "MD5 checksum"
        }
    }
    
    # Save manifest locally
    manifest_path = "/Users/siddharth/Downloads/ETL/ETL/sdtm_workspace/ae_s3_upload_manifest.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)
    
    print(f"‚úÖ Manifest generated: {manifest_path}")
    print(f"\nüìã Manifest Summary:")
    print(f"   Files: {manifest['file_count']}")
    print(f"   Total Size: {manifest['total_size_mb']} MB")
    print(f"   Base Path: {manifest['s3_base_path']}")
    
    # Return manifest for agent processing
    return manifest


if __name__ == "__main__":
    manifest = asyncio.run(main())
    
    print("\n" + "=" * 80)
    print("‚úÖ Upload Preparation Complete")
    print("=" * 80)
    print("\nNext Steps:")
    print("1. Execute upload_to_s3 tool calls for each file")
    print("2. Upload manifest file to S3")
    print("3. Verify all uploads with HEAD requests")
    print("4. Generate comprehensive upload report")
    print("=" * 80)
